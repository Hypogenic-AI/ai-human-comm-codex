# Downloaded Papers

1. [SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF](2310.05344_steerlm_attribute_conditioned_sft.pdf)  
   - Authors: Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, Oleksii Kuchaiev  
   - Year: 2023  
   - arXiv: 2310.05344  
   - Why relevant: Introduces attribute-conditioned supervised fine-tuning that allows controllable helpfulness/verbosity without RLHF, directly tackling steerable AI-to-human communication.

2. [HELPSTEER: Multi-attribute Helpfulness Dataset for STEERLM](2311.09528_helpsteer_multi_attribute_helpfulness.pdf)  
   - Authors: Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, et al.  
   - Year: 2023  
   - arXiv: 2311.09528  
   - Why relevant: Provides a multi-attribute preference dataset including verbosity to mitigate overlong responses; supports building concise, helpful models.

3. [Data-Centric Human Preference with Rationales for Direct Preference Alignment](2407.14477_direct_preference_alignment_rationales.pdf)  
   - Authors: Hoang Anh Just, Ming Jin, Anit Sahu, Huy Phan, Ruoxi Jia  
   - Year: 2024  
   - arXiv: 2407.14477  
   - Why relevant: Adds rationales to preference pairs to improve alignment efficiency; applicable to teaching models explicit brevity/clarity rationales for better human-facing communication.

4. [Group Preference Optimization: Few-Shot Alignment of Large Language Models](2310.11523_group_preference_optimization.pdf)  
   - Authors: Siyan Zhao, John Dang, Aditya Grover  
   - Year: 2024 (ICLR)  
   - arXiv: 2310.11523  
   - Why relevant: Meta-learns group-specific preference steering, enabling personalization of response length/style for different audiences.

5. [Instructive Dialogue Summarization with Query Aggregations](2310.10981_instructive_dialogue_summarization.pdf)  
   - Authors: Bin Wang, Zhengyuan Liu, Nancy F. Chen  
   - Year: 2023  
   - arXiv: 2310.10981  
   - Why relevant: Instruction-tuned dialogue summarization with controllable word budgets; strong baseline for concise, user-focused outputs.

6. [Training language models to follow instructions with human feedback](2203.02155_instructgpt_human_feedback.pdf)  
   - Authors: Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, et al.  
   - Year: 2022  
   - arXiv: 2203.02155  
   - Why relevant: Foundational RLHF work (InstructGPT) showing preference learning improves helpfulness and reduces verbosity/toxicity; establishes alignment baseline.
